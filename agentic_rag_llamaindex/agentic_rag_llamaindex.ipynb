{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building an autonomous research agent\n",
    "- rag with llamaindex\n",
    "- 1. routing: add decision making to route requests to multiple tools\n",
    "\n",
    "- 2. tool use: create an interface for the agents to select a tool and generate the right argument for that tool\n",
    "\n",
    "- 3. multi-step reasoning with a range of tool using the LLM while maintaining language throughout that process\n",
    "\n",
    "- 4. let users optionally inject guidance during intermediate steps (during reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nest_asyncio # make jupyter nb play nicelly with llamaindex\n",
    "from helper import load_api_key\n",
    "OPENAI_API_KEY = load_api_key('openai_pat.txt')\n",
    "OPENAI_ORG_KEY = load_api_key('openai_org.txt')\n",
    "import openai \n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Set up the OpenAI client\n",
    "# client = OpenAI(\n",
    "#     api_key=OPENAI_API_KEY,\n",
    "#     organization=OPENAI_ORG_KEY\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test API connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham has also written several influential essays on technology, startups, and entrepreneurship.', additional_kwargs={}, raw=ChatCompletion(id='chatcmpl-ACzfKF8IQI2pMnZ6qc3HthTMyYt5H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham has also written several influential essays on technology, startups, and entrepreneurship.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727660342, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=51, prompt_tokens=11, total_tokens=62, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0))), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test API connection\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "resp = OpenAI().complete(\"Paul Graham is \")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "resp = llm.stream_complete(\"Paul Graham is \")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ahoy matey! The name's Captain Rainbowbeard, the most colorful pirate on the seven seas! What can I do for ye today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = OpenAI().chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & parse data, define indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# read pdf into a parsed documentation # ICRL 2024 multi-agent paper\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the documents into even sized chucks\n",
    "# split on the order of sentences\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# define documents into nodes \n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global confile setting \n",
    "# specify the LLM embedding model to use \n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from utils import *\n",
    "\n",
    "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\") \n",
    "# Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\") \n",
    "Settings.embed_model =OpenAIEmbedding(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two indexes over these nodes: a summary index & a vector index \n",
    "# (index is a set of meta data over our data - can query an index, and different indexes have different retrieval behaviors) \n",
    "\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define query engines and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn indexes into query engines and query tools: turn indexes into query engines \n",
    "# query engines are derived from each of the query indexes\n",
    "\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "\n",
    "\n",
    "# turn indexes into query engines and query tools: set metadata (query tool)\n",
    "# a query tool for each of the query engine\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Router \n",
    "- llm powered\n",
    "- pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM powered single selector\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which aligns with the purpose of summarization questions related to MetaGPT..\n",
      "\u001b[0mThe document introduces MetaGPT, a meta-programming framework designed for multi-agent collaboration using Large Language Models (LLMs). MetaGPT incorporates Standardized Operating Procedures (SOPs) to streamline workflows and reduce errors in complex tasks. It assigns specific roles to agents, such as Product Manager, Architect, Engineer, and QA Engineer, to break down tasks into manageable subtasks. The framework uses structured communication interfaces and a publish-subscribe mechanism to enhance efficiency and minimize information overload. MetaGPT also includes an executable feedback mechanism to iteratively improve code quality during runtime. The framework has demonstrated state-of-the-art performance on benchmarks like HumanEval and MBPP, and it excels in handling complex software development tasks compared to other frameworks like AutoGPT and ChatDev. The document also discusses the potential for self-improvement mechanisms and multi-agent economies to further enhance MetaGPT's capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific context about how agents share information, which aligns with retrieving specific context from the MetaGPT paper..\n",
      "\u001b[0mAgents share information with other agents by using a shared message pool to publish structured messages. They can also subscribe to relevant messages based on their profiles, allowing them to obtain directional information from other roles and public information from the environment.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How do agents share information with other agents?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put all above into a module: get_router_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"metagpt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'Tell me about the ablation study results?' is asking for specific context from the MetaGPT paper, which aligns with choice 2..\n",
      "\u001b[0mThe ablation study results demonstrate the effectiveness of MetaGPT in addressing challenges related to context utilization, reducing hallucinations in software generation, and managing information overload. The study highlights how MetaGPT's unique designs successfully tackle issues such as ambiguity in natural language descriptions, maintaining information validity in lengthy contexts, reducing code hallucinations, and handling information overload through a global message pool and subscription mechanism.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
